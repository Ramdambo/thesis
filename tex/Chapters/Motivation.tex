% Chapter 1
\chapter{Introduction} % Main chapter title

\label{ch:Intro} % For referencing the chapter elsewhere, use \ref{Chapter1}

Before we dive in, let me first explain what this topic is about and try to motivate the thought
process behind it as well shortly explain what you are going to read about on the next pages.

\section{Motivation}
\label{sec:Motivation}

As technology evolves, the quality and resolution of digital images improve as well. But as the
quality increases so does the memory required to store the image on a hard drive. To counteract
this increase in disk space usage, people have tried to reduce the sizes of digital images a lot in the
last decades.\\
One of the most successful and probably most well known \textit{codecs}
is \textbf{JPEG} and its successor \textbf{JPEG 2000}. Both are lossy image compression methods
known for fairly high compression rates while still providing a reasonably image quality.\\
For
higher compression rates however, the quality deteriorates pretty quickly and the infamous ``block
artifacts'' are being introduced. As a remedy, a new method for image compression has been developed in the last years that
aims to create better looking images for higher compression rates than JPEG and even JPEG2000. \\
This new method roughly works by selecting a small amount of pixels to keep and then filling in
the gaps in the reconstruction/decompression step.\\
As one can imagine, selecting the right data is a fairly minute process and one has to carefully
select the pixels to keep. Even though there has been a lot of work done in this area, the
selection can still be improved.\\
In the past, the usefulness of corners for this process was proven in~\cite{zimmer07} even though the
method proposed in this work would not surpass JPEG's abilities. Nonetheless, we want to build on
it and explore
how keeping larger regions of data around corners plays out in this process.

\section{Related Work}

Next up, we are going to discuss some work related to this thesis to see what advances have been done
in the last years and what our work is built upon.

\subsection{Inpainting}

Inpainting as a technique is nothing new, it existed since a long time in the form of e.g.
restoration of old images and film. In 2000, Bertalmio et al first proposed an algorithm to digitally
inpaint images without user intervention. After consulting actual experts in image restoration they
came up with a method imitating human restorators.\\
The main idea behind their method is to continue the structure
surrounding the gap into it and simultaneously fill in the different regions in each gap with the colour at its
boundaries\cite{bertalmio00}.\\
\textbf{TODO:} Explaing inpainting\\
Although it produces good looking images without obvious artifacts, it lacks the ability to
reproduce texture. Furthermore, they proposed to use second order PDEs instead of the high order
PDEs they used to solve the inpainting problem. Galić et al touched on this issue in 2008,
proposing to use EED because of its inpainting capabilities as a replacement for the higher order
PDE\cite{galic08}.
However, they only used EED for inpainting instead of interleaving a PDE based inpainting approach
with a mean curvature motion model as proposed in \cite{bertalmio00}. This was featured in another
work that I will cover in the next section.

\subsection{PDE-based image compression}
In 2005, Galić et al.\ first introduced an alternative image compression method using PDE-based inpainting as
a serious alternative to more classical approaches like JPEG and JPEG 2000~\cite{galic05}. In this
work, the authors showed the inpainting capabilities of nonlinear anisotropic diffusion, specifically of
a diffusion process called \textit{edge-enhancing diffusion}, or short EED.\@
The specifics of this process will be covered in~\ref{sec:Diffusion}.\\
For data selection they used an
\textit{adaptive sparsification scheme relying on B-tree triangular coding (BTTC)}, hence the name
\textit{BTTC-EED}~\cite{galic05}, as an easy to implement and fast compression method
\cite{distasi97}.
With this fairly simple approach they were already able to outperform JPEG visually for high
compression rates and comic-style images~\cite{galic05}.\\
Improving on this, the authors published a new paper in 2008, adding a number of additional
procedures to the compression phase, with which they were finally able to come close to the quality
of JPEG 2000~\cite{galic08}.
Finally, in 2009, Schmaltz et al.\ optimised the ideas even further, building the
so called \textbf{R-EED} codec with which they could even beat JPEG 2000~\cite{schmaltz09}.
The main differences between~\cite{galic05} and~\cite{schmaltz09} are the addition of several
procedures to optimise the data set that is kept for inpainting in the decompression step.
To roughly summarise the whole compression phase~\cite{schmaltz09}:\\
First, an initial set of points is gathered by using a rectangular subdivision (instead of the
previous triangular subdivision) of the image. This works by recursively splitting the image in
half whenever the reconstruction using only the boundary points exceeds a certain error threshold.
The reconstruction is also done using EED inpainting.
After obtaining the initial data set, the brightness values of each of the kept pixels is rescaled
to $[0, 255]$ to eliminate possible quantisation artifacts. Due to this brightness rescaling, the
optimal contrast parameter for the decompression phase may change and thus has to be adjusted as
well. This is generally done alternating between optimising points and the contrast
parameter until a certain convergence criterion is met.
As a last step, the authors invert the inpainting mask and perform the inpainting process to fill in
    the kept data as a means to increase the coherence between the optimised pixels and the
    original image.

    All of these measures serve the purpose of decreasing the \textit{mean squared error (MSE)} to a
level where the proposed codec is able to outperform JPEG 2000 for compression rates higher than
$\mathbf{43:1}$.

\subsection{Image features in Inpainting}

Semantically, edges and corners are the most important features of an image. Because of this, there
are multiple publications trying to exploit the semantic importance of these features for image
inpainting. For example in 2010, Mainberger et al published a paper on the reconstruction of images
using only relevant edges and homogeneous diffusion. Building on their work from 2009, the authors
were able to successfully reconstruct cartoonish images from only a set of edges they detectedj
using the Marr-Hildreth edge detector\cite{hildreth85}.
In contrast to other inpainting methods that rely on sparse images as their inpainting domain 
and therefore need to use more sophisticated PDEs in order to successfully reconstruct the image, 
the algorithm described in this work is built around a simple homogeneous diffusion equation\cite{mainberger10}. 
Their reasoning behind this is that homogeneous diffusion is \textit{one of the analytically best 
understood inpainting approaches}(\cite{mainberger10}) as well as, because of its simplicity, 
computationally the least challenging out of all PDE-based approaches.

Another approach by Zimmer that I already mentioned in \ref{sec:Motivation} proved the importance of corners
for image inpainting in image compression\cite{zimmer07}. It is the Even though they could not beat the quality of JPEG, it
still serves as a valuable foundation for future work. Their approach was to create a sparse image
from a set of what they called \textit{corner regions} which essentially is the set of pixels
directly neighbouring a cornerd detected by the F\"orstner-Harris corner
detector\cite{harris88}. For the inpainting in the decompression phase, they came up with a more
sophisticated version of EED-based inpainting by interleaving it with \textit{Mean Curvature Motion
    (MCM)} which, in the past, has proven itself valuable especially for inpainting larger regions
\cite{bertalmio00}.

\section{Outline}

The thesis is organised as follows:\\
First, I will introduce some mathematical concepts such as the structure tensor and diffusion
processes in \ref{ch:Theory} as well as talk about the general theory behind this topic.
Afterwards in \ref{ch:Implementation}, we will discuss discretisation strategies and how the
parameters for corner detection and inpainting were chosen.
In \ref{ch:Experiments}, I will shortly go over the testing framework I implemented to more efficiently
generate test images and simultaneously test the procedure on these images and then show some
examples.
Last, but not least, we will discuss the shortcomings and future work in \ref{ch:Conclusion}.
