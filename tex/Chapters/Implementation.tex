\chapter{Implementation}

\label{ch:Implementation} 

In this chapter I will go over the technical details of the implementation. We will start with the
discretisation of the theory introduced in chapter \ref{ch:Theory}. The topic after this deals with
the selection of that various parameters of both the corner detection and inpainting algorithms and
what methods were introduced to simplify the selection or at least make it more intuitive.

\section{Discretisation}

Since reality is not infinitely fine, things such as infinitesimal calculations as seen in 
calculus, e.g. differentation of functions, can not be applied to the real world directly.
This is a problem, because digital images are inherently not continuous as they contain only a
finite number of pixels. 
We could have also solved the theoretical problem in a discrete domain but that would have 
been much more troublesome.
That is why we rather develop a continuous theory and then discretise it later to actually
implement the ideas as algorithms.

\subsection{Discrete Images}
Let $f:\Omega \rightarrow \mathbb{R}$ be an image where $\Omega
:= (0, n_x)\times(0, n_y) \subset \mathbb{R}^2$ as defined in section \ref{sec:Basics}. To
\textit{sample} the image, i.e. to discretise the image domain, we assume that all pixels lie on a
rectangular equidistant grid inside $\Omega$, where each cell in the grid has a size of $h_x
\times h_y$.
That yields $N_x := n_x/h_x$ pixels in x- and $N_y := n_y/h_y$ pixels in
y-direction.
That being said, we define the pixel $u_{i,j}$ at grid location $(i, j)^\top$ as
\begin{equation}
    u_{i, j} := u(ih_x, jh_y)\qquad \forall(i ,j) \in \{1,\dots,N_x\}\times\{1,\dots,N_y\}
\end{equation}
With that approach, the pixels are defined to lie on the crossing of the grid lines.
An alternative idea defines the pixels to lie in the centre of each cell, i.e. at location 
$((i-\frac{1}{2})h_x,\ (j- \frac{1}{2})h_y)^\top$.
As a sidenote, the cell sizes in either direction are pretty much always assumed to be 1 in 
practice. 
But to keep the theory as universal as possible, we will use $h_x$ and $h_y$ instead.
Sampling of the spatial domain is not the only step necessary to discretise an image. We also have
to discretise the \textit{co-domain} or \textit{grey-value-domain}. In theory our grey value domain
is just $\mathbb{R}$, but since this is rather unpractical, we quantise the grey value range, i.e.
limit it to $[0, 255]$.

\subsection{Numerical Differentiation}

Image derivatives are essential to image processing as seen in the previous chapter. Therefore we
need a way to compute them even on discrete images. To compute the gradient or in the simpler case
just the derivative of a discrete function, one generally uses so called \textit{finite difference
schemes}. Such a scheme is normally derived from the \textit{Taylor expansion} of the continuous
function. For example, we want to compute the first derivative of a 1D function $f:\mathbb{R}
\rightarrow \mathbb{R}$.
The Taylor expansion of \textit{degree $n$} of this function around the point $x_0\in\mathbb{R}$ is given by 
\begin{equation}
    f(x) = T_n(x, x_0) + \mathcal{O}(h^{n+1})
\end{equation}
where $\mathcal{O}(h^{n+1})$ describes the magnitude of the leading error term and as such the
\textit{approximation quality} of the Taylor series.
The actual Taylor series is defined as
\begin{equation}
    T_n(x, x_0) = \sum\limits_{k=0}^{n} \frac{(x-x_0)^k}{k!}f^{(k)}(x_0)
    \footnote{$f^{(k)}$ denotes the $k$-th derivative of the function $f$}
\end{equation}
A finite difference scheme generally uses a weighted sum of neighbouring values to compute the
desired derivative expression. In our example, we want to derive a scheme to compute the first
derivative of $f_i$ using its neighbours $f_{i-1}$ and $f_{i+1}$, i.e.
\begin{equation}
    f_i' \approx \alpha f_{i-1} + \beta f_i + \gamma f_{i+1}
\end{equation}
We can now describe $f_{i-1}$ and $f_{i+1}$ in terms of their Taylor expansion around $f_i$: 
\begin{align}
    f_{i-1} &= f((i-1)h) \notag\\
            &= T_n((i-1)h, ih) + \mathcal{O}(h^{n+1})\notag\\
            &= \sum\limits_{k=0}^{n}\frac{(-h)^k}{k!}f_i^{(k)}+ \mathcal{O}(h^{n+1})\\
    f_{i+1} &= \dots = \sum\limits_{k=0}^{n}\frac{h^k}{k!}f_i^{(k)}+ \mathcal{O}(h^{n+1})
\end{align}
If we now choose a concrete value for $n$ (here $n=5$) we can actually compute the approximation:
\begin{align}
    f_{i-1} &= f_i - hf_i' + \frac{h^2}{2}f_i'' - \frac{h^3}{6}f_i''' + \frac{h^4}{24}f_i'''' -
    \frac{h^5}{120}f_i''''' + \mathcal{O}(h^6)\label{eq:fi-1}\\
    f_{i+1} &= f_i + hf_i' + \frac{h^2}{2}f_i'' + \frac{h^3}{6}f_i''' + \frac{h^4}{24}f_i'''' +
    \frac{h^5}{120}f_i''''' + \mathcal{O}(h^6)\label{eq:fi+1}
\end{align}
The next step is the \textit{comparison of coefficients}, we insert
\eqref{eq:fi-1} and \eqref{eq:fi+1} into the equation 
and solve the arising linear system of equations for $\alpha,\beta,\gamma$.
\begin{align}
    0\cdot f_i + 1\cdot f_i' + 0\cdot f_i'' \overset{!}{=} \alpha f_{i-1} + \beta f_i + \gamma
    f_{i+1}
\end{align}
After the substitution, the right hand side becomes
\begin{align}
    &\alpha \left(f_i - hf_i' + \frac{h^2}{2}f_i''\right) + \beta f_i + \gamma\left(f_i + hf_i' + \frac{h^2}{2}f_i''\right)\notag\\
    = &\left(\alpha+\beta+\gamma\right)f_i + h\left(-\alpha+\gamma\right)f_i' + \frac{h^2}{2}\left(\alpha+\gamma\right)f_i''
\end{align} 
Note that for the comparison of coefficients it suffices to use the first 3 summands of the
approximation.
The linear system defined by the above equation
\begin{equation}
    \begin{pmatrix}
        1&1&1\\
        -1&0&1\\
        1&0&1
    \end{pmatrix}
    \begin{pmatrix}
        \alpha\\
        \beta\\
        \gamma
    \end{pmatrix}
    =
    \begin{pmatrix}
        0\\
        \frac{1}{h}\\
        0
    \end{pmatrix}
\end{equation}
has the solutions $\alpha = -\frac{1}{2h}, \beta = 0, \gamma = \frac{1}{2h}$.
This yields the approximation 
\begin{equation}
    f_i'\approx\frac{f_{i+1} - f_{i-1}}{2h}
\end{equation}
To find out how good this scheme is, we re-insert \eqref{eq:fi-1} and \eqref{eq:fi+1} to get
\begin{align}
    \frac{f_{i+1} - f_{i-1}}{2h}= -&\frac{1}{2h}\left(f_i - hf_i' + \frac{h^2}{2}f_i'' - \frac{h^3}{6}f_i''' + \frac{h^4}{24}f_i'''' -
    \frac{h^5}{120}f_i''''' + \mathcal{O}(h^6)\right) + \notag\\
     &\frac{1}{2h}\left(f_i - hf_i' + \frac{h^2}{2}f_i'' - \frac{h^3}{6}f_i''' + \frac{h^4}{24}f_i'''' -
    \frac{h^5}{120}f_i''''' + \mathcal{O}(h^6)\right)\notag
\intertext{Expanding and simplifying yields}
        \frac{f_{i+1} - f_{i-1}}{2h} &= f_i' + \underbrace{\frac{h^2}{6}f_i'' + \frac{h^4}{30}f_i'''' +
        \mathcal{O}(h^5)}_\text{quadratic leading error term}\notag\\
            \Rightarrow \frac{f_{i+1} - f_{i-1}}{2h} &= f_i' + \mathcal{O}(h^2)
\end{align}
This means that the error of our approximation is quadratic in the grid size. 
We also say that this approximation has a \textit{consistency order} of 2. Note that for such an
appapproximation to be reasonable, it has to have at least consistency order 1. Otherwise, it is
not guaranteed that the error term diminishes if we send the grid size $h$ to 0.
\subsection{Numerical Schemes for Diffusion}

\section{Parameter Selection}
\subsection{Corner Detection}
\subsection{Inpainting}

